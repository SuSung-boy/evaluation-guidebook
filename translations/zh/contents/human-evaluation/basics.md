# 基础概念

## 什么是人工评估？
人工评估是指让人类评价模型输出回答的好坏。
本文讨论的都是后验评估，即模型已经完成训练，给定一个任务让人类进行评估。

### 系统化评估
系统化的人工评估主要有 3 种方式：

如果你手头 **没有现成的数据集**，但还是想测试一些模型的能力，可以采用人工评估：提供一个任务说明和打分指南 (例如：`尝试与模型交互，迫使模型输出不当语言，即包含冒犯性、歧视性、暴力等。如果模型输出了不当语言，则得分为 0，反之为 1。`)，以及可供交互的测试模型，然后就可以让标注员人工操作并评分，同时列出评分理由。

如果你手头 **已经有数据集** (例如 `收集了一组 prompt，并确保这些 prompt 不会迫使模型输出不当回答`)，可以自行将 prompt 输入模型得到输出，然后将输入 prompt、输出回答、打分指南一起提供给标注员评估 (`如果模型意外输出不当，则得分为 0，反之为 1`)。

如果你手头 **既有数据集也有评分结果**，可以让人工标注员通过 [错误注释](https://ehudreiter.com/2022/06/01/error-annotations-to-evaluate/) 的方法 (*这种方法同样可以作为评估系统，适用于上面的情况*) 来对评估进行审查。在测试新评估系统时，这一步非常重要，但是技术测层面属于对评估系统的评估，因此略微超出本文的讨论范围。

注：
- *如要对已部署的生产模型做评估，可以考虑进行人工 A/B测试及反馈。*
- *[AI 审计 (AI audits)](https://arxiv.org/abs/2401.14462) (模型外部系统评估) 也是一种人工评估方式，但不在本文讨论范围。

### 非正式评估
基于人类的评估方法还有两种不那么正式的方法：

**Vibes 检查** 是一种使用非公开数据进行人工评估的方法，用来在多个场景用例 (如代码编程和文学创作等) 上测试来把握整体效果。评估结果通常会被当作轶事证据而分享在 Twitter 和 Reddit 上，不过它们很容易受到主观认知偏差的影响 (换句话说，人们往往只相信自己相信的结果)。尽管如此，这些结果依然能作为 [你自己测试的一个不错起点](https://olshansky.substack.com/p/vibe-checks-are-all-you-need)。

**Arenas** 是一种众包人工评估的方法，用来给多个模型表现排名。
一个知名的例子是 [LMSYS 聊天机器人 Arena 评估](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard), 社区用户通过与多个模型对话来分辨孰优孰劣并投票。总的投票结果将汇总为 Elo 排名 (这场多个模型比赛的排名)，来评判出 “最优模型”。
## 人工评估的优劣势

优势：
- **灵活性**：只要评估定义的足够明确，人工评估几乎适用于所有任务！
- **无数据污染**：人工书写的问题 prompt 不会跟训练集有交叉 (希望如此)。
- **与人类偏好的相关性**：这条显而易见，毕竟是按人工标准来评分的。
  *注：进行人工评估时，尽量确保标注员的多样性，以保证评估结果的泛化性。*

劣势：
- **第一印象偏差**：人工标注员往往根据 [第一印象](https://arxiv.org/abs/2309.16349) 来评估回答的质量，有时候会忽略对事实的考证。
- **语气偏差**：众包标注员对语气特别敏感，容易低估一些表述比较坚定的句子而出现事实或逻辑错误。比如模型以自信的语气说出错误的内容，标注员可能很难发觉，进而导致对输出更为自信的模型的评分偏高。相比之下，专家标注员受语气偏差的影响更低。
- **自我偏好偏差**：人们有时候会 [偏向于选择迎合自己观点的答案](https://arxiv.org/abs/2310.13548)，而不是事实正确的答案。
- **身份背景偏差**：不同身份背景的人具有不同的价值观，可能导致评估模型时表现出显出差异 (例如在模型输出的 [不当回答评估](https://arxiv.org/abs/2205.00501) 中，对何为不当表述的理解偏差)。
### 系统化人工评估
系统化人工评估 (尤其是付费的人工) 的优势：
- **高质量数据**：可以根据评估任务量身定制测试集，为你开发 (例如需要开发偏好模型) 和评估模型提供进一步支持。
- **数据隐私**：付费标注员 (尤其是内部人员) 通常很注重数据安全性，反而 LLM 评估的闭源 API 模型的数据隐私性较差，因为你需要将个人数据发送给外部服务。
- **可解释性**：标注员在评分时会清晰的说明打分理由。

缺点：
- **成本较高**：当然你需要支付给标注员费用。甚至为了优化评估指南，你还需要多轮迭代，这会使得费用更高。
- **扩展性差**: ：除非你的评估任务非常依赖用户反馈，否则人工评估方法的扩展性确实不太好，因为每次进行一轮新的评估都需要重新调动人员 (并支付费用)。
- **重复性低**：除非你能保证每次评估都是同一批标注员并且评分标准完全明确，否则不同的标注员对评估结果的可能无法精确复现。

### 非正式人工评估
优势：
- **成本较低**：社区成员自愿参与，费用支付较少。
- **发现边缘用例**：由于限制较少，成员自发的创造性可能会发现一些有趣的边缘用例。
- **扩展性高**：只要有足够多的社区成员自愿参与，评估的扩展性就会更好，且参与门槛较低。

劣势：
- **高度主观性**：由于社区成员的自身的 [文化局限性](https://arxiv.org/abs/2404.16019v1)，尽管标准一致，也很难在评分时保持一致性。不过 “群体智慧” 效应 (参考 Galton 的 Wiki 页面) 可能在大量的评分投票中平滑地缓解这一问题。
- **评分偏好不具代表性**：由于年轻西方男性在互联网技术社区中的占比过高，可能导致评估的偏好严重失衡，这跟实际上普通大众的口味并不一致，因此会影响评估的准确性。
- **容易被操控**：如果你请的众包标注员没经过筛选，第三方机构很容易通过操控他们来导致模型的评分异常 (如偏高)，尤其是当模型的写作风格比较独特的时候。
