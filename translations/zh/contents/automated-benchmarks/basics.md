# 基础概念

*注：本文内容与我写的 [通用评估博客](https://huggingface.co/blog/clefourrier/llm-evaluation) 存在部分重叠*
## 什么是自动评估基准？

自动评估基准通常用来评估模型在某个方面的效果。可以是一个明确定义的具体 **任务**，如 `模型在垃圾邮件分类任务上效果怎么样？`，也可以是一个更加抽象和通用的 **能力**，如 `模型的数学能力如何？`

自动评估基准包括：
- **测试样本** 组成的 **数据集**
	- 测试样本包含模型输入，有时候还附带用来与模型输出对比的参考答案。
	- 通常样本的设计标准是模拟要测试模型的内容：例如，邮件分类任务需要构建一个邮件数据集，其中的样本就要涵盖垃圾邮件和正常邮件，或者具有挑战性的边缘用例等。
- **评估指标**
	- 评估指标就是为模型表现打分的方法
	  例如：按分类正确性来评估模型分类垃圾邮件的能力 (正确分类的样本得 1 分，错误分类的样本得 0 分)。
	- 对于 LLM，通常对模型的输出进行打分来作为评估指标，主要有两种输出：
		- 文本输出。即模型生成的文本，可采用 *生成式评估*
		- 对数概率输出。即模型生成的一个或多个序列的概率值，可采用 *多项选择评估* (也叫 MCQA，或 *困惑度评估*)。
		- 可以点击 [模型推理与评估](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/general-knowledge/model-inference-and-evaluation.md) 页面了解更多信息。

在模型没有见过 (即未出现在训练集) 的数据上进行评估会更有意义，得出的模型 **泛化性** 结论才更准确。比如在只见过假冒银行垃圾邮件的模型上测试其能否正确分类与 “健康” 相关的垃圾邮件。

注：*模型只能在训练数据上预测效果良好 (没有隐式地学习到更高层次的通用范式) 的现象叫做 **过拟合**。这就类似于一个学生死记硬背了考试题目，却没有理解背后的知识点。所以只用训练集中的数据测试评估 LLM 得到的分数指标实际上是模型不具备的能力。*

## 自动评估基准的优劣势
优势：
- **一致性和可重复性**：在同一个模型上运行相同的自动评估基准 10 次，测试结果也是相同的 (除非受到硬件或模型自身随机性的影响)。所以相同任务下，多个模型的测试排名结果是公正的。
- **低成本规模效益**：目前自动评估基准是评估模型成本最低的方式之一。
- **易于理解**：大部分自动化方式的评价指标理解起来都非常容易。
  *例如：精确匹配可以理解为生成文本跟参考文本是否完全一致；准确率可以理解为做出的选项有多大程度是正确的 (不过对于像 `BLEU` 或 `ROUGE` 这种评价方式，理解难度会稍微高一些)。*
- **高质量测试集**：许多自动评估基准的测试集都来自专家级生成数据集或现有的高质量数据集 (如 MMLU 或 MATH)。当然也不是说这些测试集就完美无瑕，例如 MMLU 就被发现存在一些解析错误以及事实谬误，所以后来出现了一批改进的数据集，如 MMLU-Pro 和 MMLU-Redux。

劣势：
- **复杂任务难以保证效果**：自动评估基准通常在测试效果容易定义和评估的任务上表现良好 (如分类任务)。一旦任务比较复杂而且难以拆分为目标明确的子任务时，表现可能不及预期。
  *例如：测试模型的 “数学能力” 任务。具体是算术、还是逻辑、亦或是推演新数学概念的能力？*
  所以出现了一些无需拆分为子任务的 **通用性** 评估方式，由此评估出的模型整体表现就是评估目标的 **优良代理**。 
- **数据污染**：网络上的数据一旦以纯文本的形式公开，那么由于数据爬虫，这些数据总归会出现在模型训练集中。所以在评估时很难保证模型真的没有见过测试集。
