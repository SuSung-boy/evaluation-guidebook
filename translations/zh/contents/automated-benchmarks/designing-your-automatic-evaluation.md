# 设计你的自动评估任务 

## 选择数据集
做评估时，你可以选择现有的数据集 (参考 [一些评估数据集](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/automated-benchmarks/some-evaluation-datasets.md) 页面) 作为测试集，也可以设计自己的数据集。有一点非常重要，请注意：**评估的结果与评估的数据集质量高度相关**。

### 使用现有的数据集
这部分强烈建议仔细阅读！
#### 数据集需要注意的问题
样本是由谁创建的？
在我看来，按照样本的标注员素质高低，数据集质量大致排名如下：专家构建数据集 > 付费标注数据集 > 众包数据集 > MTurk 数据集。
你可以在数据集的说明文档 (data card) 找到标注员的统计信息，可以帮助理解数据集语言多样性。

- **样本是否经过其他标注员或作者的审核？** 
你需要先弄明白：
	- 不同标注员标注结果是否一致？
	-  完整数据集是否经过作者审核？
标注员通常不是目标语言的母语使用者（例如 AWS Mechanical Turk），否则可能会出现拼写错误、语法错误或无意义的答案。

- **是否给标注员提供了明确的数据创建指导？**
换句话说，数据集样本间的标注标准是否一致？

#### 检查样本  
随机抽取 50 个样本进行人工检查：
- *检查质量*：
	- 问题是否明确且不含歧义？
	- 对应的回答是否正确？(*例如：TriviaQA 的每个问题通常包含多个标准答案，有时这些答案会相互冲突。*)
	- 信息是否完整？(*例如: MMLU 有许多问题中缺少参考示意图。*)
- *检查与任务相关性*：
	- 样本问题是否是 LLM 特定评估任务的问题类型？
	- 样本是否与测试用例相关？

数据集样本数量同样重要 (以确保自动评估基准结果在统计上显著，一般至少需要 100 个测试样本)。
### 设计自己的数据集
有 3 种设计方法：
#### 整合数据
要使用自己的测试集评估模型执行特定任务的能力，可以从不同的现成数据源整理和聚合。实际上有许多评估测试集都是以这种方式构建的，例如 MATH 和 LSAT 就聚合了人工评估数据集。当然在整理数据时，请遵循上文的质量与任务相关性检查步骤。
#### 人工标注
关于 `人工标注` 的内容，本指南有一整个篇幅详细介绍，可以自行点击 [Using human annotators](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/human-evaluation/using-human-annotators.md) 阅读。
#### 合成数据
- **使用 LLM 合成**
这部分可以参考 HF 员工的 [Cosmopedia](https://huggingface.co/blog/cosmopedia) 博客！虽然此篇主要研究如何构建训练集，但想法和技术同样适用于构建测试集。
合成的测试集仍需手动检查 (遵循上文步骤)。

- **基于规则合成**
如果任务允许，这个绝佳的方法几乎可以无限获取测试样本，并且避免数据污染。
参考 [NPHardEval](https://arxiv.org/abs/2312.14890)，[DyVal](https://arxiv.org/abs/2309.17167)，[MuSR](https://arxiv.org/abs/2310.16049), [BabiQA](https://arxiv.org/abs/1502.05698) 等。

## 选择推理方法
除了测试集，还需要选择合适的推理方法。

对于多项选择问答任务 (通常用于测试模型的知识储备或消除歧义的能力)，使用对数概率 (MCQA) 非常有效。
- 优势： 
	- 可以保证所有模型都能获取正确答案。
	- 能够提供模型 “置信度” 代理 (以及校准)。
	- 评估速度快，尤其是单 token 预测任务时 (选择索引 A/B/C/D 或 Yes/No 等)。
	- 允许获取小模型在任务表现上的信号。
- 劣势：
	- 可能高估小模型的表现。如果不做限制，会使得模型生成的内容超出可选范围。
	- 估结果可能不具代表性。一些模型 [倾向于按多项选择的顺序生成特定选择](https://arxiv.org/abs/2309.03882)。

对于测试模型流畅性、推理或回答问题能力的任务，使用 QA 生成非常有效。
- 优势：
	- 与人类关心的点一致，即 LLM 生成文本是否流畅的能力。
- 劣势：
	- 可能存在评分困难 (见下面的 `度量标准` 部分)。
	- 成本比对数似然评估稍高，尤其是需要采样的任务。

## 选择 prompt
Prompt 设计关键问题：
- 提供给模型的关于任务的信息量大小
- 如何向模型提供信息

MCQA 或 QA 任务的通用 prompt 设计范式一般包含以下几个部分：
- 任务 prompt (可选)：描述任务。
- 上下文：为问题提供额外的背景信息。
	- *例如: 对于内容总结或信息提取任务，可以提供内容来源*
- 问题：prompt 的核心内容。
- 对于多项选择评估任务，可以增加选项。
- 连接词 (`问题`、`上下文`、`选项`等)。

定义 prompt 时需要注意：
- 在语义等价的 prompt 中，即使非常微小的变化也可能导致巨大差异的结果 (详见 [Troubleshooting reproducibility](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/troubleshooting/troubleshooting-reproducibility.md) 的 `Different prompt` 部分)，并且 prompt 格式也可能对特定模型的输出造成影响。
	- 如何缓解这一问题：
		- 高成本方法：使用不同的 prompt 变体进行多次评估。
		- 低成本方法：使用多种 prompt 格式分别分配给多个等效难度的测试样本进行单次评估。
- 在 prompt 中提供示例可以帮助模型输出遵循预期格式，示例可以通过连接词添加至 prompt。
- 注意模型可能倾向于对特定的 prompt 格式过拟合。
	- [这篇论文](https://arxiv.org/abs/2407.07890) 对此有更详尽的探讨，文中展示了一些模型因在测试集 **格式** 上过拟合而导致的评估分数过高的情况。
	- 我们特别观察到，在 Open LLM Leaderboard 2 上, Llama 3.2 和 Qwen 2.5 出于这个原因已经不再提供 few-shot 示例的 prompt 格式。
- 对于一些测试任务的指标，你可能希望模型的输出限制在一个小范围。
  *可以跳转 [Model inference and evaluation](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/general-knowledge/model-inference-and-evaluation.md) 页面的 `Constraining model outputs` 部分了解更多信息。*

## 选择评估指标
如果你关注 **对数概率** 评估，那么你期望的度量指标会很简单：准确率 (选择最佳选项的频率)。如果在这个基础上你还想要进行标准化 (通过长度、字符、token 或 PMI)，那么度量指标就会变成困惑度 (perplexity)、召回率或 F1 分数。

对于 **生成式** 评估，你期望的度量指标范围会更广。
为此你需要：
1. 确定生成结果的度量顺序，是直接拿生成结果比较，还是先使用某种方式进行标准化。
	- 标准化如果设计不当，评估结果会有失偏颇 (参考这篇 [博客](https://huggingface.co/blog/open-llm-leaderboard-drop))。但总的来说，它们都能在任务层面提供信号。
	- 标准化对某些特定任务 (例如数学能力评估) 非常重要，因为你可能需要从格式化输出中提取有效的结果。
	- 如果你想要通过添加机制 (如思维链) 来评估准确率，那么标准化同样重要，因为你需要将推理轨迹从实际结果中去除。
2. 确定生成结果与参考答案的比较方式。
   你可以采用任意的比较方法。评估匹配程度的有：精确匹配、前缀匹配等；评估摘要和翻译能力的有：ROUGE、BLEU、n-gram 等。更多评价指标可以点击 [这个页面](https://github.com/huggingface/lighteval/wiki/Metric-List) 查看，我会在后续更新关于在何时使用哪种指标的章节。

总的来说，选择哪种评价指标取决于你的任务内容。对于某些领域 (如医疗、聊天机器人)，你可能不想要评估平均性能，而是需要评估 **最差表现** (如医疗知识输出质量、如果输出不实的后果等)。(*可以查看 [这篇博客](https://ehudreiter.com/2024/07/10/challenges-in-evaluating-llms/) 深入了解*)

## 智能新任务：功能性测试是什么？
对于代码领域，显然仅评估生成代码的语义是不够的，必须测试代码实际运行情况。所以需要专门设计一个功能性测试：对于给定 prompt 生成的代码段，测试并评估其是否能正确通过单元测试。

这种功能性测试方法极具前景，因为：
- 使得生成测试用例更容易 (大部分情况下都可以基于规则生成测试用例)
- 减少过拟合
- 可以评估模型的特定主动能力

不过很多新奇的想法需要一些创造性的工作才能实现！

IFEval 是一个不错的例子，它是用来测试模型指令遵循能力的评估基准，通过创建多个格式化指令 (*例如：添加指定数量的特殊符号，仅将一句话字母大写，等等*) 并严格测试生成结果的遵循与否。功能性测试的想法仍需更多的工作来扩展到其他的特征测试上！
